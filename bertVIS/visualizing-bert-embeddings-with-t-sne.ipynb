{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of us used BERT embeddings in this competition. I think I'm not wrong when I say that, for most of us, it's a bit mysterious precisely what information about the text gets encoded in BERT's layers. So I decided to make some visualizations of the embeddings, using a dimensional reduction algorithm called [t-SNE](https://lvdmaaten.github.io/tsne/). Here's a sketch:\n",
    "1. For BERT-large, each word is represented as a vector in a 1024-dimensional space;\n",
    "2. Using a good old PCA, we reduce this to a 50-dimensional space, hopefully without losing too much information;\n",
    "3. Using PCA to further reduce from 50 to 2 dimensions would probably kill a lot of useful information, so we want a more refined method. What t-SNE does, roughly, is create vectors in a 2-dimensional space, such that if two vectors have small distance in the 50-dimensional space, they also have small distance in the 2-dimensional space. We get a 2-dimensional plot, which offers a little insight into how BERT embeddings for various words are distributed.\n",
    "\n",
    "I have a concrete question that I'd like to answer using these plots. I think a lot of people noticed that you can concatenate different layers of BERT, not necessarily the last ones. For my team, what worked best was concatenating layers -4, -5, -6. We will talk more about our solution elsewhere. But just to give you an idea, here are some experiments which I did with the model from [my previous kernel](https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline). After replacing BERT-base with BERT-large, and concatenating embeddings coming from two layers only, I get the following results:\n",
    "\n",
    "With layers -5, -6:\n",
    "CV mean score: 0.4666, std: 0.0278.\n",
    "Test score: 0.41730251922932554\n",
    "\n",
    "With layers -3, -4:\n",
    "CV mean score: 0.4929, std: 0.0267.\n",
    "Test score: 0.45579418221937407\n",
    "\n",
    "With layers -1, -2:\n",
    "CV mean score: 0.5311, std: 0.0205.\n",
    "Test score: 0.49026846792574713\n",
    "\n",
    "It's pretty clear that layers -5, -6 are much better suited for this problem than the first 4. So in the graphs below, I took the first 10 examples from gap-development, and I'm plotting the result of t-SNE for layer -1, and separately for layer -5. Hopefully staring long enough at plots like these can reveal something about the different ways in which BERT layers encode information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd \n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading just 10 examples from the gap-development file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-04 11:12:44--  https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1080993 (1.0M) [text/plain]\n",
      "Saving to: ‘gap-development.tsv’\n",
      "\n",
      "gap-development.tsv 100%[===================>]   1.03M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-04 11:12:45 (7.39 MB/s) - ‘gap-development.tsv’ saved [1080993/1080993]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n",
    "nrows = 10\n",
    "data = pd.read_csv(\"gap-development.tsv\", sep = '\\t', nrows = nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-04 11:12:45--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.210.176, 216.58.209.176, 216.58.209.208, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.210.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1247797031 (1.2G) [application/zip]\n",
      "Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n",
      "\n",
      "uncased_L-24_H-1024 100%[===================>]   1.16G  53.3MB/s    in 28s     \n",
      "\n",
      "2022-08-04 11:13:13 (42.8 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#downloading weights and cofiguration file for bert\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "with zipfile.ZipFile(\"uncased_L-24_H-1024_A-16.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "!rm \"uncased_L-24_H-1024_A-16.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# !wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodeling\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mextract_features\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenization\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/work/transformers_prepare/BookTransformers/bertVIS/modeling.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBertConfig\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;124;03m\"\"\"Configuration for `BertModel`.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n",
    "# !wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
    "\n",
    "import modeling\n",
    "import extract_features\n",
    "import tokenization\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def compute_offset_no_spaces(text, offset):\n",
    "\tcount = 0\n",
    "\tfor pos in range(offset):\n",
    "\t\tif text[pos] != \" \": count +=1\n",
    "\treturn count\n",
    "\n",
    "def count_chars_no_special(text):\n",
    "\tcount = 0\n",
    "\tspecial_char_list = [\"#\"]\n",
    "\tfor pos in range(len(text)):\n",
    "\t\tif text[pos] not in special_char_list: count +=1\n",
    "\treturn count\n",
    "\n",
    "def count_length_no_special(text):\n",
    "\tcount = 0\n",
    "\tspecial_char_list = [\"#\", \" \"]\n",
    "\tfor pos in range(len(text)):\n",
    "\t\tif text[pos] not in special_char_list: count +=1\n",
    "\treturn count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the 10 GAP examples through BERT, and saving layers -1, -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "text = data[\"Text\"]\n",
    "text.to_csv(\"input.txt\", index = False, header = False)\n",
    "\n",
    "os.system(\"python3 extract_features.py \\\n",
    "  --input_file=input.txt \\\n",
    "  --output_file=output.jsonl \\\n",
    "  --vocab_file=uncased_L-24_H-1024_A-16/vocab.txt \\\n",
    "  --bert_config_file=uncased_L-24_H-1024_A-16/bert_config.json \\\n",
    "  --init_checkpoint=uncased_L-24_H-1024_A-16/bert_model.ckpt \\\n",
    "  --layers=-1,-5 \\\n",
    "  --max_seq_length=256 \\\n",
    "  --batch_size=8\")\n",
    "\n",
    "bert_output = pd.read_json(\"output.jsonl\", lines = True)\n",
    "os.system(\"rm output.jsonl\")\n",
    "os.system(\"rm input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "emb_2d = {}\n",
    "for row in range(nrows):\n",
    "    P = data.loc[row,\"Pronoun\"].lower()\n",
    "    A = data.loc[row,\"A\"].lower()\n",
    "    B = data.loc[row,\"B\"].lower()\n",
    "    P_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"Pronoun-offset\"])\n",
    "    A_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"A-offset\"])\n",
    "    B_offset = compute_offset_no_spaces(data.loc[row,\"Text\"], data.loc[row,\"B-offset\"])\n",
    "    # Figure out the length of A, B, not counting spaces or special characters\n",
    "    A_length = count_length_no_special(A)\n",
    "    B_length = count_length_no_special(B)\n",
    "    \n",
    "    # Get the BERT embeddings for the current line in the data file\n",
    "    features = pd.DataFrame(bert_output.loc[row,\"features\"]) \n",
    "    \n",
    "    span = range(2,len(features)-2)\n",
    "    emb1, emb5 = {}, {}\n",
    "    count_chars = 0\n",
    "    \n",
    "    # Make a list with the text of each token, to be used in the plots\n",
    "    texts = []\n",
    "\n",
    "    for j in span:\n",
    "        token = features.loc[j,'token']\n",
    "        texts.append(token)\n",
    "        emb1[j] = np.array(features.loc[j,'layers'][0]['values'])\n",
    "        emb5[j] = np.array(features.loc[j,'layers'][1]['values'])\n",
    "        if count_chars == P_offset:\n",
    "            texts.pop()\n",
    "            texts.append(\"@P\" + token)\n",
    "        if count_chars in range(A_offset, A_offset + A_length): \n",
    "            texts.pop()\n",
    "            if data.loc[row,\"A-coref\"]:\n",
    "                texts.append(\"@G\" + token)\n",
    "            else:\n",
    "                texts.append(\"@R\" + token)\n",
    "        if count_chars in range(B_offset, B_offset + B_length): \n",
    "            texts.pop()\n",
    "            if data.loc[row,\"B-coref\"]:\n",
    "                texts.append(\"@G\" + token)\n",
    "            else:\n",
    "                texts.append(\"@R\" + token)\n",
    "        count_chars += count_length_no_special(token)\n",
    "    \n",
    "    X1 = np.array(list(emb1.values()))\n",
    "    X5 = np.array(list(emb5.values()))\n",
    "    if row == 0: print(\"Shape of embedding matrix: \", X1.shape)\n",
    "\n",
    "    # Use PCA to reduce dimensions to a number that's manageable for t-SNE\n",
    "    pca = PCA(n_components = 50, random_state = 7)\n",
    "    X1 = pca.fit_transform(X1)\n",
    "    X5 = pca.fit_transform(X5)\n",
    "    if row == 0: print(\"Shape after PCA: \", X1.shape)\n",
    "\n",
    "    # Reduce dimensionality to 2 with t-SNE.\n",
    "    # Perplexity is roughly the number of close neighbors you expect a\n",
    "    # point to have. Our data is sparse, so we chose a small value, 10.\n",
    "    # The KL divergence objective is non-convex, so the result is different\n",
    "    # depending on the seed used.\n",
    "    tsne = TSNE(n_components = 2, perplexity = 10, random_state = 6, \n",
    "                learning_rate = 1000, n_iter = 1500)\n",
    "    X1 = tsne.fit_transform(X1)\n",
    "    X5 = tsne.fit_transform(X5)\n",
    "    if row == 0: print(\"Shape after t-SNE: \", X1.shape)\n",
    "    \n",
    "    # Recording the position of the tokens, to be used in the plot\n",
    "    position = np.array(list(span)) \n",
    "    position = position.reshape(-1,1)\n",
    "    \n",
    "    X = pd.DataFrame(np.concatenate([X1, X5, position, np.array(texts).reshape(-1,1)], axis = 1), \n",
    "                     columns = [\"x1\", \"y1\", \"x5\", \"y5\", \"position\", \"texts\"])\n",
    "    X = X.astype({\"x1\": float, \"y1\": float, \"x5\": float, \"y5\": float, \"position\": float, \"texts\": object})\n",
    "\n",
    "    # Remove a few outliers based on zscore\n",
    "    X = X[(np.abs(stats.zscore(X[[\"x1\", \"y1\", \"x5\", \"y5\"]])) < 3).all(axis=1)]\n",
    "    emb_2d[row] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the 2-dimensional representations output by t-SNE. I labeled each datapoint by the token it represents, using blue text for the pronoun, green text for the correct coreferent, and red text for incorrect correferents. The color of the points represents the position of the token in the sentence: blue is towards the beginning, red towards the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "for row in range(nrows):\n",
    "    X = emb_2d[row]\n",
    "    \n",
    "    # Plot for layer -1\n",
    "    plt.figure(figsize = (20,15))\n",
    "    p1 = sns.scatterplot(x = X[\"x1\"], y = X[\"y1\"], hue = X[\"position\"], palette = \"coolwarm\")\n",
    "    p1.set_title(\"development-\"+str(row+1)+\", layer -1\")\n",
    "    \n",
    "    # Label each datapoint with the word it corresponds to\n",
    "    for line in X.index:\n",
    "        text = X.loc[line,\"texts\"]\n",
    "        if \"@P\" in text:\n",
    "            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='blue', weight='semibold')\n",
    "        elif \"@G\" in text:\n",
    "            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='green', weight='semibold')\n",
    "        elif \"@R\" in text:\n",
    "            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='red', weight='semibold')\n",
    "        else:\n",
    "            p1.text(X.loc[line,\"x1\"]+0.2, X.loc[line,\"y1\"], text, horizontalalignment='left', \n",
    "                    size='medium', color='black', weight='semibold')\n",
    "    \n",
    "    # Plot for layer -5\n",
    "    plt.figure(figsize = (20,15))\n",
    "    p1 = sns.scatterplot(x = X[\"x5\"], y = X[\"y5\"], hue = X[\"position\"], palette = \"coolwarm\")\n",
    "    p1.set_title(\"development-\"+str(row+1)+\", layer -5\")\n",
    "    \n",
    "    for line in X.index:\n",
    "        text = X.loc[line,\"texts\"]\n",
    "        if \"@P\" in text:\n",
    "            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='blue', weight='semibold')\n",
    "        elif \"@G\" in text:\n",
    "            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='green', weight='semibold')\n",
    "        elif \"@R\" in text:\n",
    "            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text[2:], horizontalalignment='left', \n",
    "                    size='medium', color='red', weight='semibold')\n",
    "        else:\n",
    "            p1.text(X.loc[line,\"x5\"]+0.2, X.loc[line,\"y5\"], text, horizontalalignment='left', \n",
    "                    size='medium', color='black', weight='semibold') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some useful information in these plots. Notice two reasons why points are close:\n",
    "1. They represent the same word, or similar words, independently of context, such as \"girlfriend\" and \"boyfriend\" in development-1.\n",
    "2. They represent tokens which have close positions in the sentence, such as \"episode\" and \"final\" in development-1.\n",
    "\n",
    "In some cases, you can see directly from these plots that BERT has learned some information that's very useful for coreference resolution. For example, in development-5, \"she\" and \"rivera\" are very close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend to play with these tools more, and update the kernel if I have any new insights. But let me end with a disclaimer: here's a great [explanation](https://distill.pub/2016/misread-tsne/) of some of the pitfalls of t-SNE visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
